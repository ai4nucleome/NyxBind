# üß¨NyxBind

NyxBind is a high-performance pre-trained model for transcription factor binding site (TFBS) prediction. It is built upon DNABERT2 with additional contrastive learning to enhance sequence representation for regulatory genomics.

---

## 1. Environment Setup

We recommend setting up a virtual environment using Anaconda.

### 1.1 Create and Activate Virtual Environment

```bash
conda create -n nyxbind python=3.8
conda activate nyxbind
```

### 1.2 Install the Package and Dependencies

```bash
git clone https://github.com/ai4nucleome/NyxBind.git
cd NyxBind
pip install --file requirements.txt
```

---

## üöÄ2. Download NyxBind (Pretrained, Not Fine-tuned)

You can load the pretrained NyxBind model from Hugging Face:

```python
from transformers import AutoTokenizer, AutoModel
from transformers.models.bert.configuration_bert import BertConfig

config = BertConfig.from_pretrained("CompBioDSA/NyxBind")
tokenizer = AutoTokenizer.from_pretrained("CompBioDSA/NyxBind", trust_remote_code=True)
model = AutoModel.from_pretrained("CompBioDSA/NyxBind", trust_remote_code=True, config=config)
```

---

##üìÅ 3. Dataset Structure

To fine-tune NyxBind on downstream TFBS tasks, organize your dataset in the following format:

```
--Folder/
  ‚îî‚îÄ‚îÄ TF_NAME/
      ‚îú‚îÄ‚îÄ train.csv
      ‚îú‚îÄ‚îÄ dev.csv
      ‚îî‚îÄ‚îÄ test.csv
```

Each `.csv` file should contain labeled DNA sequences, typically with `sequence` and `label` columns.

---

##üõ†Ô∏è 4. Fine-tuning on Downstream Tasks

NyxBind supports two fine-tuning modes: **full-parameter fine-tuning** and **parameter-efficient LoRA fine-tuning**.

üìåüìåüìåNote:
All settings are configured for a single GPU. If you are using multiple GPUs, you may increase the batch size accordingly.

### 4.1 Full-Parameter Fine-tuning

You can run the full-parameter fine-tuning using the following command, or modify and run `./finetune/ft/ft.sh`:

```bash
python train.py \
    --model_name_or_path $model_path \
    --data_path $data \
    --kmer -1 \
    --run_name FT_${lr}_${folder_name}_${seed} \
    --model_max_length 30 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --gradient_accumulation_steps 1 \
    --learning_rate ${lr} \
    --num_train_epochs 5 \
    --fp16 \
    --save_steps 200 \
    --output_dir output/NyxBind-FT-${lr} \
    --evaluation_strategy steps \
    --eval_steps 200 \
    --warmup_steps 30 \
    --logging_steps 100000 \
    --overwrite_output_dir True \
    --log_level info \
    --find_unused_parameters False
```

> üîß You can modify `model_path` to use DNABERT2 or NyxBind. Feel free to adjust batch size, learning rate, and other hyperparameters.

---

### 4.2 LoRA Fine-tuning

Use LoRA (Low-Rank Adaptation) for efficient fine-tuning:

```bash
python train.py \
    --model_name_or_path $model_path \
    --data_path $data \
    --kmer -1 \
    --run_name LoRA_${lr}_${folder_name}_${seed} \
    --model_max_length 30 \
    --use_lora \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_target_modules 'query,value,key,dense' \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --gradient_accumulation_steps 1 \
    --learning_rate ${lr} \
    --num_train_epochs 5 \
    --fp16 \
    --save_steps 100 \
    --output_dir output/NyxBind-LoRA${lr} \
    --evaluation_strategy steps \
    --eval_steps 100 \
    --warmup_steps 30 \
    --logging_steps 100000 \
    --overwrite_output_dir True \
    --log_level info \
    --seed ${seed} \
    --find_unused_parameters False
```

> üß™ LoRA enables training with fewer trainable parameters. You can tune `lora_r`, `lora_alpha`, and target modules as needed.

---

##üîç 5. Motif Visualization and Extraction

This section details the process of visualizing and extracting sequence motifs from attention scores generated by the fine-tuned NyxBind model.

# The `motif` Folder: A Closer Look at its Contents

The `motif` folder serves as the central hub for all operations related to motif visualization, extraction, and benchmarking. Here‚Äôs a breakdown of its key sub-files and sub-directories:

- **33JASPAR**  
  This sub-directory contains JASPAR data, specifically organized for visualizing 33 predefined motifs. JASPAR is a widely recognized open-access database of curated transcription factor binding profiles.

- **attention_output**  
  You'll find the attention scores generated by the fine-tuned NyxBind model stored here. These scores are crucial for deriving meaningful sequence motifs.

- **find_motifs.py**  
  This Python script is designed to identify and extract motifs from processed data.

- **meme**  
  This sub-directory serves as a storage location for MEME (Multiple Em for Motif Elicitation) outputs, including both newly generated and existing motifs used in analysis.

- **motif_benchmark**  
  This folder contains the necessary data and scripts for benchmarking the motif generation and fine-tuning processes of various models, such as BertSNR, DeepSNR, and D_AEDNet.

- **motif.sh**  
  This bash script automates the process of generating motifs directly from the attention scores and their corresponding sequences. this script run `find_motifs.py` and `motif_utils.py`.

- **motif_utils.py**  
  This Python utility script provides common functions and helper classes that are used across other motif-related scripts, streamlining tasks like data processing or motif manipulation.

- **result**  
  This is the output directory where the final generated motif logos (visual representations of sequence motifs) and PFMs (Position Frequency Matrices) are saved.

- **score_from_sft.py**  
  This Python script is specifically dedicated to extracting attention scores from the model's output.

- **sft.sh**  
  A bash script that manages the entire attention score extraction process, likely executing `score_from_sft.py`.



### 5.1 Extract attention score

After obtaining attention scores using `sft.sh`, you can visualize the learned motifs derived from the attention maps.

Ensure the following files are available:
- `./33JASPAR/<TF_NAME>/motif.csv`: original sequence file
- `../finetune/ft/output/NyxBind-33-ft/weights/<TF_NAME>`: ft models

To convert the attention scores into interpretable sequence motifs, run the visualization script:

```bash
python score_from_sft.py \
    --model_path path/to/finetuned_model \
    --data_path path/to/sequence_data \
    --output_root path/to/save_attention_scores \
    --selected_layers 11 \
    --batch_size 256 \
    --max_length 30
```

### 5.2 Extracting Motifs

Ensure the following files are available:
- `./attention_output/NyxBind/<TF_NAME>/atten.npy`: attention score file
- `./33JASPAR/<TF_NAME>/motif.csv`: original sequence file

To further analyze and identify motifs, you can use the `motif.sh` script. An example of its usage is:

```bash
python find_motifs.py \
    --data_dir path/to/sequence_data \
    --predict_dir path/to/attention_score_folder  \
    --window_size 11 \
    --min_len 6 \
    --top_k 1 \
    --pval_cutoff 0.005 \
    --min_n_motif 10 \
    --align_all_ties \
    --save_file_dir path/to/save_folder \
    --verbose
```
The parameter top_k = 1 indicates that we select the most frequently occurring motif as the final representative motif. Although the default window_size is set to 11 for easier comparison, it can be extended up to 24 to potentially improve performance by capturing longer motif patterns.

This script analyzes the specified data and prediction directories to identify motifs based on configurable parameters such as window size, minimum motif length, p-value cutoff, and the minimum number of motifs to extract. The --save_file_dir argument determines the output directory where the discovered motifs will be saved.

### 5.3 TomTom Compariso

#### Preparation

Before running the comparison, you need to convert Position Frequency Matrices (PFMs) into PWM MEME format files using the provided Jupyter notebook:

Ensure the following files are available:
-'../result/NyxBind/<TF_name>/PFMfile.jaspar': PFM file.
results will be saved to './motif/meme/NyxBind/<TF_name>.meme'

```bash
.motif/meme/transfer-meme.ipynb
```

#### TOMTOM
Ensure the following files are available:
-'./human/human.meme': merged all existing human TFBS meme file.
-'./motif/meme/NyxBind/<TF_name>.meme' : meme files for comparision.
The script `./motif/meme/tom.sh` is used to compare generated motifs against all known human TFBS motifs represented as Position Weight Matrices (PWMs).

#### Results
After run tom.sh
Filered results will be saved in ./motif/meme/filter_res


# NyxBind
