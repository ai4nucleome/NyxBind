# NyxBind

NyxBind is a high-performance pre-trained model for transcription factor binding site (TFBS) prediction. It is built upon DNABERT2 with additional contrastive learning to enhance sequence representation for regulatory genomics.

---

## 1. Environment Setup

We recommend setting up a virtual environment using Anaconda.

### 1.1 Create and Activate Virtual Environment

```bash
conda create -n nyxbind python=3.8
conda activate nyxbind
```

### 1.2 Install the Package and Dependencies

```bash
git clone https://github.com/ai4nucleome/NyxBind.git
cd NyxBind
pip install --file requirements.txt
```

---

## 2. Download NyxBind (Pretrained, Not Fine-tuned)

You can load the pretrained NyxBind model from Hugging Face:

```python
from transformers import AutoTokenizer, AutoModel
from transformers.models.bert.configuration_bert import BertConfig

config = BertConfig.from_pretrained("CompBioDSA/NyxBind")
tokenizer = AutoTokenizer.from_pretrained("CompBioDSA/NyxBind", trust_remote_code=True)
model = AutoModel.from_pretrained("CompBioDSA/NyxBind", trust_remote_code=True, config=config)
```

---

## 3. Dataset Structure

To fine-tune NyxBind on downstream TFBS tasks, organize your dataset in the following format:

```
--Folder/
  â””â”€â”€ TF_NAME/
      â”œâ”€â”€ train.csv
      â”œâ”€â”€ dev.csv
      â””â”€â”€ test.csv
```

Each `.csv` file should contain labeled DNA sequences, typically with `sequence` and `label` columns.

---

## 4. Fine-tuning on Downstream Tasks

NyxBind supports two fine-tuning modes: **full-parameter fine-tuning** and **parameter-efficient LoRA fine-tuning**.

### 4.1 Full-Parameter Fine-tuning

You can run the full-parameter fine-tuning using the following command, or modify and run `./finetune/ft/ft.sh`:

```bash
python train.py \
    --model_name_or_path $model_path \
    --data_path $data \
    --kmer -1 \
    --run_name FT_${lr}_${folder_name}_${seed} \
    --model_max_length 30 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --gradient_accumulation_steps 1 \
    --learning_rate ${lr} \
    --num_train_epochs 5 \
    --fp16 \
    --save_steps 200 \
    --output_dir output/NyxBind-FT-${lr} \
    --evaluation_strategy steps \
    --eval_steps 200 \
    --warmup_steps 30 \
    --logging_steps 100000 \
    --overwrite_output_dir True \
    --log_level info \
    --find_unused_parameters False
```

> ðŸ”§ You can modify `model_path` to use DNABERT2 or NyxBind. Feel free to adjust batch size, learning rate, and other hyperparameters.

---

### 4.2 LoRA Fine-tuning

Use LoRA (Low-Rank Adaptation) for efficient fine-tuning:

```bash
python train.py \
    --model_name_or_path $model_path \
    --data_path $data \
    --kmer -1 \
    --run_name LoRA_${lr}_${folder_name}_${seed} \
    --model_max_length 30 \
    --use_lora \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_target_modules 'query,value,key,dense' \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --gradient_accumulation_steps 1 \
    --learning_rate ${lr} \
    --num_train_epochs 5 \
    --fp16 \
    --save_steps 100 \
    --output_dir output/NyxBind-LoRA${lr} \
    --evaluation_strategy steps \
    --eval_steps 100 \
    --warmup_steps 30 \
    --logging_steps 100000 \
    --overwrite_output_dir True \
    --log_level info \
    --seed ${seed} \
    --find_unused_parameters False
```

> ðŸ§ª LoRA enables training with fewer trainable parameters. You can tune `lora_r`, `lora_alpha`, and target modules as needed.

---

## 5\. Motif Visualization and Extraction

This section details the process of visualizing and extracting sequence motifs from attention scores generated by the fine-tuned NyxBind model.

# The `motif` Folder: A Closer Look at its Contents

The `motif` folder serves as the central hub for all operations related to motif visualization, extraction, and benchmarking. Hereâ€™s a breakdown of its key sub-files and sub-directories:

- **33JASPAR**  
  This sub-directory contains JASPAR data, specifically organized for visualizing 33 predefined motifs. JASPAR is a widely recognized open-access database of curated transcription factor binding profiles.

- **attention_output**  
  You'll find the attention scores generated by the fine-tuned NyxBind model stored here. These scores are crucial for deriving meaningful sequence motifs.

- **find_motifs.py**  
  This Python script is designed to identify and extract motifs from processed data.

- **meme**  
  This sub-directory serves as a storage location for MEME (Multiple Em for Motif Elicitation) outputs, including both newly generated and existing motifs used in analysis.

- **motif_benchmark**  
  This folder contains the necessary data and scripts for benchmarking the motif generation and fine-tuning processes of various models, such as BertSNR, DeepSNR, and D_AEDNet.

- **motif.sh**  
  This bash script automates the process of generating motifs directly from the attention scores and their corresponding sequences. this script run `find_motifs.py` and `motif_utils.py`.

- **motif_utils.py**  
  This Python utility script provides common functions and helper classes that are used across other motif-related scripts, streamlining tasks like data processing or motif manipulation.

- **result**  
  This is the output directory where the final generated motif logos (visual representations of sequence motifs) and PFMs (Position Frequency Matrices) are saved.

- **score_from_sft.py**  
  This Python script is specifically dedicated to extracting attention scores from the model's output.

- **sft.sh**  
  A bash script that manages the entire attention score extraction process, likely executing `score_from_sft.py`.



### 5.1 Visualizing Learned Motifs

After obtaining attention scores using `run_attention.sh`, you can visualize the learned motifs from the attention maps. Ensure that the following files are present: `./attention_output/NyxBind/<TF_NAME>/atten.npy` and `./<DATA_ROOT>/<TF_NAME>/motif.csv`. To convert attention scores into interpretable sequence motifs, use the provided visualization script:

```bash
python visualize_motif.py \
    --atten_path ./attention_output/NyxBind/CTCF/atten.npy \
    --motif_path ./33JASPAR/CTCF/motif.csv \
    --output_dir ./motif_figures/CTCF
```

The generated motif figures (e.g., heatmaps or sequence logos) will be saved in the specified `--output_dir`. Key parameters for this script include `--atten_path` (path to the `.npy` attention score file), `--motif_path` (path to the `motif.csv` file containing original sequences), and `--output_dir` (directory for saving results).

### 5.2 Extracting Motifs

To further analyze and identify motifs, you can use the `find_motifs.py` script. An example of its usage is:

```bash
python find_motifs.py \
    --data_dir "$SUBDIR" \
    --predict_dir "$PREDICTION_PATH" \
    --window_size 11 \
    --min_len 6 \
    --top_k 1 \
    --pval_cutoff 0.005 \
    --min_n_motif 10 \
    --align_all_ties \
    --save_file_dir "$MOTIF_PATH" \
    --verbose
```

This script will analyze the specified data and prediction directories to identify motifs based on parameters such as window size, minimum motif length, p-value cutoff, and the minimum number of motifs to find. The `--save_file_dir` argument specifies where the identified motifs will be stored.

### 5.3 TomTom Comparison

The script `./motif/meme/tom.sh` is used to compare generated motifs against all known human TFBS motifs represented as Position Weight Matrices (PWMs).

#### Preparation

Before running the comparison, you need to convert Position Frequency Matrices (PFMs) into PWM MEME format files using the provided Jupyter notebook:

```bash
.motif/meme/transfer-meme.ipynb
```


#### Results
After run tom.sh
Filered results will be saved in ./motif/meme/filter_res


# NyxBind
