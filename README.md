# üß¨NyxBind

NyxBind is a high-performance pre-trained model for transcription factor binding site (TFBS) prediction. It is built upon DNABERT2 with additional contrastive learning to enhance sequence representation for regulatory genomics.

---

## 1. Environment Setup

We recommend setting up a virtual environment using Anaconda.

### 1.1 Create and Activate Virtual Environment

```bash
conda create -n nyxbind python=3.8
conda activate nyxbind
```

### 1.2 Install the Package and Dependencies

```bash
git clone https://github.com/ai4nucleome/NyxBind.git
cd NyxBind
pip install --file requirements.txt
```

---

## üöÄ2. Download NyxBind (Pretrained, Not Fine-tuned)

You can load the pretrained NyxBind model from Hugging Face:

```python
from transformers import AutoTokenizer, AutoModel
from transformers.models.bert.configuration_bert import BertConfig

config = BertConfig.from_pretrained("CompBioDSA/NyxBind")
tokenizer = AutoTokenizer.from_pretrained("CompBioDSA/NyxBind", trust_remote_code=True)
model = AutoModel.from_pretrained("CompBioDSA/NyxBind", trust_remote_code=True, config=config)
```

---

##üìÅ 3. Dataset Structure

To fine-tune NyxBind on downstream TFBS tasks, organize your dataset in the following format:

```
--Folder/
  ‚îî‚îÄ‚îÄ TF_NAME/
      ‚îú‚îÄ‚îÄ train.csv
      ‚îú‚îÄ‚îÄ dev.csv
      ‚îî‚îÄ‚îÄ test.csv
```

Each `.csv` file should contain labeled DNA sequences, typically with `sequence` and `label` columns.

---

##üõ†Ô∏è 4. Fine-tuning on Downstream Tasks

NyxBind supports two fine-tuning modes: **full-parameter fine-tuning** and **parameter-efficient LoRA fine-tuning**.

üìåüìåüìåNote:
All settings are configured for a single GPU. If you are using multiple GPUs, you may increase the batch size accordingly.

### 4.1 Full-Parameter Fine-tuning

You can run the full-parameter fine-tuning using the following command, or modify and run `./finetune/ft/ft.sh`:

```bash
python train.py \
    --model_name_or_path $model_path \
    --data_path $data \
    --kmer -1 \
    --run_name FT_${lr}_${folder_name}_${seed} \
    --model_max_length 30 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --gradient_accumulation_steps 1 \
    --learning_rate ${lr} \
    --num_train_epochs 5 \
    --fp16 \
    --save_steps 200 \
    --output_dir output/NyxBind-FT-${lr} \
    --evaluation_strategy steps \
    --eval_steps 200 \
    --warmup_steps 30 \
    --logging_steps 100000 \
    --overwrite_output_dir True \
    --log_level info \
    --find_unused_parameters False
```

> üîß You can modify `model_path` to use DNABERT2 or NyxBind. Feel free to adjust batch size, learning rate, and other hyperparameters.

---

### 4.2 LoRA Fine-tuning

Use LoRA (Low-Rank Adaptation) for efficient fine-tuning:

```bash
python train.py \
    --model_name_or_path $model_path \
    --data_path $data \
    --kmer -1 \
    --run_name LoRA_${lr}_${folder_name}_${seed} \
    --model_max_length 30 \
    --use_lora \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_target_modules 'query,value,key,dense' \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --gradient_accumulation_steps 1 \
    --learning_rate ${lr} \
    --num_train_epochs 5 \
    --fp16 \
    --save_steps 100 \
    --output_dir output/NyxBind-LoRA${lr} \
    --evaluation_strategy steps \
    --eval_steps 100 \
    --warmup_steps 30 \
    --logging_steps 100000 \
    --overwrite_output_dir True \
    --log_level info \
    --seed ${seed} \
    --find_unused_parameters False
```

> üß™ LoRA enables training with fewer trainable parameters. You can tune `lora_r`, `lora_alpha`, and target modules as needed.

---

##üîç 5. Motif Visualization and Extraction

This section details the process of visualizing and extracting sequence motifs from attention scores generated by the fine-tuned NyxBind model.

# The `motif` Folder: A Closer Look at its Contents

The `motif` folder serves as the central hub for all operations related to motif visualization, extraction, and benchmarking. Here‚Äôs a breakdown of its key sub-files and sub-directories:

- **33JASPAR**  
  This sub-directory contains JASPAR data, specifically organized for visualizing 33 predefined motifs. JASPAR is a widely recognized open-access database of curated transcription factor binding profiles.

- **attention_output**  
  You'll find the attention scores generated by the fine-tuned NyxBind model stored here. These scores are crucial for deriving meaningful sequence motifs.

- **find_motifs.py**  
  This Python script is designed to identify and extract motifs from processed data.

- **meme**  
  This sub-directory serves as a storage location for MEME (Multiple Em for Motif Elicitation) outputs, including both newly generated and existing motifs used in analysis.

- **motif_benchmark**  
  This folder contains the necessary data and scripts for benchmarking the motif generation and fine-tuning processes of various models, such as BertSNR, DeepSNR, and D_AEDNet.

- **motif.sh**  
  This bash script automates the process of generating motifs directly from the attention scores and their corresponding sequences. this script run `find_motifs.py` and `motif_utils.py`.

- **motif_utils.py**  
  This Python utility script provides common functions and helper classes that are used across other motif-related scripts, streamlining tasks like data processing or motif manipulation.

- **result**  
  This is the output directory where the final generated motif logos (visual representations of sequence motifs) and PFMs (Position Frequency Matrices) are saved.

- **score_from_sft.py**  
  This Python script is specifically dedicated to extracting attention scores from the model's output.

- **sft.sh**  
  A bash script that manages the entire attention score extraction process, likely executing `score_from_sft.py`.



### 5.1 Extract attention score

After obtaining attention scores using `sft.sh`, you can visualize the learned motifs derived from the attention maps.

Ensure the following files are available:
- `./33JASPAR/<TF_NAME>/motif.csv`: original sequence file
- `../finetune/ft/output/NyxBind-33-ft/weights/<TF_NAME>`: ft models

To convert the attention scores into interpretable sequence motifs, run the visualization script:

```bash
python score_from_sft.py \
    --model_path path/to/finetuned_model \
    --data_path path/to/sequence_data \
    --output_root path/to/save_attention_scores \
    --selected_layers 11 \
    --batch_size 256 \
    --max_length 30
```

### 5.2 Extracting Motifs

Ensure the following files are available:
- `./attention_output/NyxBind/<TF_NAME>/atten.npy`: attention score file
- `./33JASPAR/<TF_NAME>/motif.csv`: original sequence file

To further analyze and identify motifs, you can use the `motif.sh` script. An example of its usage is:

```bash
python find_motifs.py \
    --data_dir path/to/sequence_data \
    --predict_dir path/to/attention_score_folder  \
    --window_size 11 \
    --min_len 6 \
    --top_k 1 \
    --pval_cutoff 0.005 \
    --min_n_motif 10 \
    --align_all_ties \
    --save_file_dir path/to/save_folder \
    --verbose
```
The parameter top_k = 1 indicates that we select the most frequently occurring motif as the final representative motif. Although the default window_size is set to 11 for easier comparison, it can be extended up to 24 to potentially improve performance by capturing longer motif patterns.

This script analyzes the specified data and prediction directories to identify motifs based on configurable parameters such as window size, minimum motif length, p-value cutoff, and the minimum number of motifs to extract. The --save_file_dir argument determines the output directory where the discovered motifs will be saved.

### 5.3 TomTom Compariso

#### Preparation

Before running the comparison, you need to convert Position Frequency Matrices (PFMs) into PWM MEME format files using the provided Jupyter notebook:

Ensure the following files are available:
-'../result/NyxBind/<TF_name>/PFMfile.jaspar': PFM file

results will be saved to './motif/meme/NyxBind/<TF_name>.meme'

```bash
.motif/meme/transfer-meme.ipynb
```

#### TOMTOM
Ensure the following files are available:

- `./human/human.meme`: A merged MEME-format file containing all known human TFBS motifs.
- `./motif/meme/NyxBind/<TF_name>.meme`: MEME-format file containing motifs generated by NyxBind for a specific transcription factor.


The script `./motif/meme/tom.sh` is used to compare the generated motifs against all known human TFBS motifs, represented as Position Weight Matrices (PWMs).

#### Results
After run tom.sh
Filered results will be saved in ./motif/meme/filter_res

### 5.4 Other Baseline Models for Motif Visualization

> We gratefully acknowledge the **BertSNR** team for providing their dataset and model architecture, which served as a valuable reference for benchmarking motif visualization performance.

The `motif_benchmark/` directory contains all baseline models, datasets, and scripts used for motif visualization comparison. Below is the structured overview:

```
motif_benchmark/
‚îú‚îÄ‚îÄ Baseline/ 
‚îÇ   ‚îú‚îÄ‚îÄ Weight/                  # Pretrained weights for DeepSNR and D-AEDNet
‚îÇ   ‚îú‚îÄ‚îÄ DeepLearning_Motif.py   # Motif generation script for DeepSNR and D-AEDNet
‚îÇ   ‚îú‚îÄ‚îÄ DeepLearning_Test.py    # Testing script for DeepSNR and D-AEDNet
‚îÇ   ‚îú‚îÄ‚îÄ DeepLearning_Train.py   # Training script for DeepSNR and D-AEDNet
‚îÇ   ‚îú‚îÄ‚îÄ Matching_method.py      # Matching baseline method
‚îÇ   ‚îú‚îÄ‚îÄ motif.sh                # Shell script to generate motif logos for DeepSNR and D-AEDNet
‚îÇ   ‚îú‚îÄ‚îÄ train.sh                # Shell script to train DeepSNR and D-AEDNet
‚îÇ
‚îú‚îÄ‚îÄ DNABERT/                    
‚îÇ   ‚îú‚îÄ‚îÄ 3-new-12w-0/            # 3-mer pretrained DNABERT model (download separately from Hugging Face)
‚îÇ   ‚îú‚îÄ‚îÄ ...                     # Other model variants
‚îÇ
‚îú‚îÄ‚îÄ Dataset/               
‚îÇ   ‚îú‚îÄ‚îÄ ChIP-seq/               # 33 preprocessed ChIP-seq datasets for finetuning
‚îÇ   ‚îú‚îÄ‚îÄ ReMap/                  # 33 datasets for motif visualization
‚îÇ   ‚îú‚îÄ‚îÄ CreateDataset.py        # Script to generate dataset files
‚îÇ   ‚îú‚îÄ‚îÄ DataLoader.py           # Data loading utilities
‚îÇ   ‚îú‚îÄ‚îÄ DataReader.py           # Data reading utilities
‚îÇ   ‚îú‚îÄ‚îÄ MyDataSet.py            # Custom dataset class
‚îÇ
‚îú‚îÄ‚îÄ Main/              
‚îÇ   ‚îú‚îÄ‚îÄ CrossValidToken.py      # Cross-validation script
‚îÇ   ‚îú‚îÄ‚îÄ GenerateMotif.py        # Motif generation using BertSNR
‚îÇ   ‚îú‚îÄ‚îÄ Predict.py              # TFBS prediction
‚îÇ   ‚îú‚îÄ‚îÄ TrainMultitask.py       # Multitask training
‚îÇ   ‚îú‚îÄ‚îÄ train.sh                # Train BertSNR model
‚îÇ   ‚îú‚îÄ‚îÄ motif.sh                # Generate motifs from BertSNR
‚îÇ
‚îú‚îÄ‚îÄ Model/                      # Model architectures
‚îÇ   ‚îú‚îÄ‚îÄ BertSNR.py              
‚îÇ   ‚îú‚îÄ‚îÄ D_AEDNet.py   
‚îÇ   ‚îú‚îÄ‚îÄ DeepSNR.py        
‚îÇ
‚îî‚îÄ‚îÄ Utils/               
    ‚îú‚îÄ‚îÄ BertViz.py              # Attention visualization tool
    ‚îú‚îÄ‚îÄ Metrics.py              # Evaluation metrics
    ‚îú‚îÄ‚îÄ MotifDiscovery.py       # Motif discovery methods
    ‚îú‚îÄ‚îÄ Shuffle.py              # Dinucleotide frequency shuffling
    ‚îú‚îÄ‚îÄ Visualization.py/       # Additional visualization tools
```

---

#### Data Processing

Use the `Dataset/CreateDataset.py` script to generate k-mer sequence files from the 33 ChIP-seq datasets. These files are used to finetune **BertSNR**, **DeepSNR**, and **D-AEDNet**.

---

#### Training

- Run `Main/train.sh` to train the **BertSNR** model.
- Run `Baseline/train.sh` to train **DeepSNR** and **D-AEDNet**.

---

#### Motif Generation

- Use `Main/motif.sh` to generate motifs using **BertSNR**.
- Use `Baseline/motif.sh` to generate motifs using **DeepSNR** and **D-AEDNet**.

---

####  Note

We provide pretrained weights for **DeepSNR** and **D-AEDNet** in the `Baseline/Weight/` folder.  
However, for **BertSNR**, you must manually download the DNABERT model from Hugging Face and train BertSNR yourself using the provided scripts.




## 6. Contrastive Learning

To begin, download the pretrained **DNABERT-2** model from Hugging Face:  
üëâ [https://huggingface.co/zhihan1996/DNABERT-2-117M](https://huggingface.co/zhihan1996/DNABERT-2-117M)

Replace the `bert_layers.py` file in the model directory with the customized version provided by **NyxBind** (also available on Hugging Face).

> We sincerely thank the DNABERT-2 team for offering a powerful genome-specific foundation. Their pretrained model served as a critical starting point for **NyxBind**‚Äôs contrastive pretraining.

---

### üìÅ Data Availability

Ensure the following data directories exist:

- `../data/653chipseq`: Contains 653 ChIP-seq datasets.  
  - Sequences from chromosomes 1‚Äì23 (excluding 11 and 12), X, and Y are used as the **training set**.  
  - Mixed sequences from chromosomes 11 and 12 are used as the **validation** and **test sets**.

- `./model/DNABERT-2-117M`: Pretrained DNABERT-2 model with NyxBind-modified `bert_layers.py`.

---

### üîß Running Contrastive Learning

To start training, run the provided script:

```bash
./cl/run.sh
```

This script executes the following command:

```bash
python cl.py \
  --train_batch_size 128 \
  --eval_batch_size 128 \
  --num_epochs 3 \
  --max_seq_length 30 \
  --random_seed 42 \
  --learning_rate 3e-5 \
  --base_path ../data/653chipseq \
  --model_name_or_path ./model/DNABERT-2-117M \
  --train_numbers 10000 \
  --test_numbers 1000 \
  --start_layer 11 \
  --model_save_root output/NyxBind
```

- `start_layer`: Specifies the starting encoder layer for contrastive learning.  
  For example, if set to `10`, it will extract embeddings from layers 10 and 11 and perform mean pooling for contrastive learning.

## 7. Benchmark

### 7.1 Models

The `evaluation/` directory includes benchmark pipelines for testing the performance of different TFBS prediction models under a unified evaluation framework.

```
evaluation/
‚îú‚îÄ‚îÄ bert-tfbs/         # BERT-TFBS evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ CBAM.py        # Convolutional Block Attention Module for enhancing BERT features
‚îÇ   ‚îú‚îÄ‚îÄ train.py       # Training script for BERT-TFBS and BERT-TFBS_N
‚îÇ   ‚îî‚îÄ‚îÄ train.sh       # Shell script to run the training pipeline
‚îÇ
‚îú‚îÄ‚îÄ cnn/               # CNN-based baseline models
‚îÇ   ‚îú‚îÄ‚îÄ cnn.py         # Simple CNN model for TFBS prediction
‚îÇ   ‚îú‚îÄ‚îÄ DanQ.py        # RNN-CNN hybrid model (DanQ architecture)
‚îÇ   ‚îú‚îÄ‚îÄ DeepBind.py    # Classic DeepBind model
‚îÇ   ‚îî‚îÄ‚îÄ cnn.sh         # Shell script to train and evaluate CNN baselines
‚îÇ
‚îî‚îÄ‚îÄ nt/                # Evaluation with Nucleotide Transformer models
    ‚îú‚îÄ‚îÄ train.py       # Training or feature extraction using NT models
    ‚îî‚îÄ‚îÄ nt.sh          # Shell script to evaluate with NT models
```

---

### 7.2 BERT-TFBS & BERT-TFBS_N

To train **BERT-TFBS** or **BERT-TFBS_N**, simply run:

```bash
./evaluation/bert-tfbs/train.sh
```

You can switch between the pretrained models by modifying the `model_path` variable in the script:

```bash
# BERT-TFBS_N: uses NyxBind as the base model
model_path='../../cl/output/NyxBind'

# BERT-TFBS: uses DNABERT2 as the base model
model_path='../../model/DNABERT-2-117M'
```

---

### 7.3 DeepBind & DanQ

We sincerely thank the **DeepSTF** team for providing a PyTorch version of these models on GitHub.

To train **DeepBind** or **DanQ**, execute:

```bash
./evaluation/cnn/cnn.sh
```

You can switch the model being used by modifying the `model_name` variable in the script:

```bash
model_name='DeepBind'  # Options: 'DeepBind' or 'DanQ'
```

---

### 7.4 NT Series

To evaluate the **Nucleotide Transformer** (NT) series, run:

```bash
./evaluation/nt/nt.sh
```

You can specify which NT model to use by passing an index as an argument:

```bash
bash nt.sh 0   # nucleotide-transformer-v2-500m-multi-species
bash nt.sh 1   # nucleotide-transformer-500m-human-ref
bash nt.sh 2   # nucleotide-transformer-2.5b-1000g
bash nt.sh 3   # nucleotide-transformer-2.5b-multi-species
```

Make sure the corresponding pretrained models are downloaded and accessible via the appropriate paths in the script.

---

### üìå Notes

- All models share a consistent data format for training and evaluation.
- Metrics include ROC-AUC, PR-AUC, accuracy, precision, recall, and F1-score.
- Motif visualization results (if applicable) will be saved in each model‚Äôs respective `output/` directory.


